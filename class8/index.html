<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Class 8: Testing of Deep Networks  &middot; secML</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="description" content="" />

<meta name="keywords" content="">


<meta property="og:title" content="Class 8: Testing of Deep Networks  &middot; secML ">
<meta property="og:site_name" content="secML"/>
<meta property="og:url" content="https://secml.github.io/class8/" />
<meta property="og:locale" content="en-us">


<meta property="og:type" content="article" />
<meta property="og:description" content=""/>
<meta property="og:article:published_time" content="2018-03-23T00:00:00Z" />
<meta property="og:article:modified_time" content="2018-03-23T00:00:00Z" />

  

  
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@" />
<meta name="twitter:creator" content="@" />
<meta name="twitter:title" content="Class 8: Testing of Deep Networks" />
<meta name="twitter:description" content="" />
<meta name="twitter:url" content="https://secml.github.io/class8/" />
<meta name="twitter:domain" content="https://secml.github.io">
  

<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "Article",
    "headline": "Class 8: Testing of Deep Networks",
    "author": {
      "@type": "Person",
      "name": "http://profiles.google.com/+?rel=author"
    },
    "datePublished": "2018-03-23",
    "description": "",
    "wordCount":  3179 
  }
</script>


<link rel="canonical" href="https://secml.github.io/class8/" />

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://secml.github.io/touch-icon-144-precomposed.png">
<link href="https://secml.github.io/favicon.png" rel="icon">

<meta name="generator" content="Hugo 0.71.0" />

  <!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link href='https://fonts.googleapis.com/css?family=Merriweather:300%7CRaleway%7COpen+Sans' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="https://secml.github.io/css/font-awesome.min.css">
<link rel="stylesheet" href="https://secml.github.io/css/style.css">


  

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</head>
<body>
  <main id="main-wrapper" class="container main_wrapper has-sidebar">
    <header id="main-header" class="container main_header">
  <div class="container brand">
  
  
<div class="container topline">
  <a href="/">
  Security and Privacy of Machine Learning

</a>
</div>


</div>

  <nav class="container nav primary no-print">
  


  
<a href="https://secml.github.io/syllabus">Syllabus</a>

<a href="https://secml.github.io/schedule">Schedule</a>

<a href="https://secml.github.io/teams">Teams</a>

<a href="https://secml.github.io/topics">Topics</a>

<a href="https://secml.github.io/post" title="Show list of posts">Posts</a>


</nav>

<div class="container nav secondary no-print">
  





















</div>


  

</header>


<article id="main-content" class="container main_content single">
  <header class="container hat">
  <h1><center>Class 8: Testing of Deep Networks
</center></h1>

  <div class="metas">
<time datetime="2018-03-23">23 Mar, 2018</time>


  
    &middot; by Team Panda
  
  <br>
  


</div>

</header>

  <div class="container content">
  <h2 id="deepxplore-automated-whitebox-testing-of-deep-learning-systems">DeepXplore: Automated Whitebox Testing of Deep Learning Systems</h2>
<blockquote>
<p>Kexin Pei, Yinzhi Cao, Junfeng Yang, Suman Jana. 2017. <em>DeepXplore: Automated Whitebox Testing of Deep Learning Systems</em>. In Proceedings of ACM Symposium on Operating Systems Principles (SOSP ’17). ACM, New York, NY, USA, 18 pages. <a href="https://arxiv.org/pdf/1705.06640.pdf">[PDF]</a></p>
</blockquote>
<p>As deep learning is increasingly applied to security-critical domains, having high confidence in the accuracy of a model&rsquo;s predictions is vital.  Just as in traditional software development, confidence in the correctness of a model&rsquo;s behavior stems from rigorous testing across a wide variety of possible scenarios.  However, unlike in traditional software development, the logic of deep learning systems is learned through the training process, which opens the door to many possible causes of unexpected behavior, like biases in the training data, overfitting, underfitting, etc.  As this logic does not exist as an actual line of code, deep learning models are extremely difficult to test, and those who do are are faced with two key challenges:</p>
<ol>
<li>How can all (or at least most) of the model&rsquo;s logic be triggered so as to discover incorrect behavior?</li>
<li>How can such incorrect behavior be identified without manual inspection?</li>
</ol>
<p>To address these challenges, the authors of this paper first introduce <em>neuron coverage</em> as a measure of how much of a model&rsquo;s logic is activated by the test cases. To avoid manually inspecting output behavior for correctness, other DL systems designed for the same purpose are compared across the same set of test inputs, following the logic that if the models disagree than at least one model&rsquo;s output must be incorrect.  These two solutions are then reformulated into a joint optimization problem, which is implemented in the whitebox DL-testing framework DeepXplore.</p>
<h3 id="limitations-of-current-testing">Limitations of Current Testing</h3>
<p>The motivation for the DeepXplore framework is the inability of current methods to thoroughly test deep neural networks.  Most existing techniques to identify incorrect behavior require human effort to manually label samples with the correct output, which quickly becomes prohibitively expensive for large datasets.  Additionally, the input space of these models is so large that test inputs cover only a small fraction of cases, leaving many corner cases untested.  Recent work has shown that these untested cases near model decision boundaries leave DNNs vulnerable to adversarial evasion attacks, in which small perturbations to the input cause a misclassification.  And even when these adversarial examples are used to retrain the model and improve accuracy, they still do not have enough model coverage to prevent future evasion attacks.</p>
<h3 id="neuron-coverage">Neuron Coverage</h3>
<p>To measure the area of the input space covered by tests, the authors define what they call &ldquo;neuron coverage,&rdquo; a metric analogous to code coverage in traditional software testing.  As seen in the figure below, neuron coverage measures the percentage of nodes a given test input activates in the DNN, analogous to the percentage of the source code executed on code coverage metrics.  This is believed to be a better measure of the robustness test inputs because the logic of a DNN is learned, not programmed, and exists primarily in the layers of nodes that compose the model, not the source code.</p>
<!-- raw HTML omitted -->
<h3 id="cross-referencing-oracles">Cross-referencing Oracles</h3>
<p>To eliminate the need for expensive human effort to check output correctness, multiple DL models are tested on the same inputs and their behavior compared.  If different DNNs designed for the same application produce different outputs on the same input, at least one of them should be incorrect, therefore identifying potential model inaccuracies.</p>
<!-- raw HTML omitted -->
<h3 id="method">Method</h3>
<p>The primary objective for the test generation process is to maximize the neuron coverage and differential behaviors observed across models.  This is formulated as a joint optimization problem with domain-specific constraints (i.e., to ensure that a test case discovered is still a valid input), which is then solved using a gradient ascent algorithm.</p>
<h3 id="experimental-set-up">Experimental Set-up</h3>
<p>The DeepXplore framework was used to test three DNNs for each of five well-known public datasets that span multiple domains: MNIST, ImageNet, Driving, Contagio/Virustotal, and Drebin.  Because these datasets include images, video frames, PDF malware, and Android malware, different domain-specific constraints were incorporated into DeepXplore for each dataset (e.g. pixel values for images need to remain between 0 and 255, PDF malware should still be a valid PDF file, etc.).  The details of the chosen DNNs and datasets can be seen in the table below.</p>
<!-- raw HTML omitted -->
<p>For each dataset, 2,000 random samples were selected as seed inputs, which were then manipulated to search for erroneous behaviors in the test DNNs.  For example, in the image datasets, the lighting conditions were modified to find inputs on which the DNNs disagreed. This is shown in the photos below from the Driving dataset for self-driving cars.</p>
<!-- raw HTML omitted -->
<h2 id="deep-k-nearest-neighbors-towards-confident-interpretable-and-robust-deep-learning">Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning</h2>
<blockquote>
<p>Nicolas Papernot, Patrick McDaniel. 2018. <em>Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning</em>. <a href="https://arxiv.org/pdf/1803.04765.pdf">[PDF]</a></p>
</blockquote>
<h2 id="k-nearest-neighbors">k-Nearest Neighbors</h2>
<p>Deep learning is ubiquitous. Deep neural networks achieve a good performance on challenging tasks like machine translation, diagnosing medical conditions, malware detection, and classification of images. In this research work, the authors mentioned about three
well-identified criticisms directly relevant to the security. They are the lack of reliable confidence in machine learning, model interpretability and robustness. Authors introduced the Deep k-Nearest Neighbors (DkNN) classification algorithm in this research work. It enforces the conformity of the predictions made by a DNN model on the test data with respect to the model’s training data. For each layer in the neural network, the DkNN performs a nearest neighbor search to find training points for which the layer’s output is closest to the layer’s output on the test input. Then they analyze the assigned label of these neighboring points to make it sure that the intermediate layer&rsquo;s computations remain conformal with the final output model’s prediction.</p>
<!-- raw HTML omitted -->
<p>Consider a deep neural network (in the left of the figure), representations output by each layer (in the middle of the figure) and the nearest neighbors found at each layer in the training data (in the right of the figure). Drawings of pandas and school buses indicate training points. We can observe that confidence is high when there is homogeneity among the nearest neighbors labels. Interpretability of the outcome of each layer is provided by the nearest neighbors. Robustness stems from detecting nonconformal predictions from nearest neighbor labels found for out-of-distribution inputs across different layers.</p>
<h3 id="algorithm">Algorithm</h3>
<p>The psudo-code for their k-Nearest Neighbors (DkNN) that the authors introduced in ensuring that the intermediate layer&rsquo;s computations remain conformal with the respect to the final model&rsquo;s prediction is given below-</p>
<!-- raw HTML omitted -->
<h2 id="basis-for-evaluation">Basis for Evaluation</h2>
<p>Basis for evaluating robustness, interpretability, Confidence are discussed below-</p>
<!-- raw HTML omitted -->
<h2 id="evaluation-of-confidence-credibility">Evaluation of Confidence/ Credibility</h2>
<p>In their experiments, they measured high confidence on inputs. From the experiment they observed that credibility varies across both in- and out-of-distribution samples. They tailored their evaluation to demonstrate that the credibility is well calibrated. They performed their experiments on both benign and adversarial examples.</p>
<h2 id="classification-accuracy">Classification Accuracy</h2>
<p>In their experiment, they used three datasets. First, hand written recognition task of MNIST dataset, SVHN dataset and the third one is GTSRB dataset. In the following figure, we can observe the comparison of the accuracy between DNN and DkNN model on three different dataset.</p>
<!-- raw HTML omitted -->
<h2 id="credibility-on-in-distribution-samples">Credibility on in-distribution samples</h2>
<p>Reliability diagrams are plotted for the three different datasets (MNIST, SVHN and GTSRB) below:</p>
<!-- raw HTML omitted -->
<p>On the left, they visualized the estimation of confidence output by the DNN softmax and it is calculated by the probability \(arg ~max_{j} ~f_j(x)\). On the right, they plotted the credibility of DkNN predictions. From the graph, it may appear that the softmax is better calibrated than the corresponding DkNN. Because its reliability diagrams are closer to the linear relation between accuracy and DNN confidence. But if the distribution of DkNN credibility values are considered then it surfaces that the softmax is almost always very confident on test data with a confidence above 0.8. DkNN uses the range of possible credibility values for datasets like SVHN (test set contains a larger number of inputs that are difficult to classify).</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="credibility-on-out-of-distribution-samples">Credibility on out-of-distribution samples</h2>
<p>Images from NotMNIST is identical to MNIST but the classes are non-overlapping. For MNIST, the first set of out-of-distribution samples contains images from the NotMNIST dataset. For SVHN, the out-of-distribution samples contains images from the CIFAR-10 dataset. Again, they have the same format but there is no overlap between SVHN and CIFAR-10. For both the MNIST and SVHN datasets, they rotated all the test inputs by an angle of 45 degree to generate a second set of out-of-distribution samples.</p>
<!-- raw HTML omitted -->
<p>In the above figure, the credibility of the DkNN on the out-of-distribution samples is compared with the DNN softmax on MNIST (left) and SVHN (right). The DkNN algorithm has an average credibility of 6% and 9% to inputs from the NotMNIST and rotated MNIST test sets respectively, compared to 33% and 31% for the softmax probabilities. We find the same observation for SVHN model. Here, the DkNN assigns an average credibility of 15% and 18% to CIFAR-10 and rotated SVHN inputs, compared to 52% and 33% for the softmax probabilities.</p>
<h2 id="evaluation--of-the-interpretability">Evaluation  of the interpretability</h2>
<p>Here, they have considered the model being bias to the skin color of a person. In a recent study, Stock and Cisse demonstrate how an image of former US president Barack Obama throwing an American football in a stadium ResNet model. They reproduce their experiment and apply the DkNN algorithm to this model. They plotted the 10 nearest neighbors from the training data. These neighbors are computed by the last hidden layer of ResNet model.</p>
<!-- raw HTML omitted -->
<p>On the left side of the above figure, the test image that is processed by the DNN is the same as that of the one used by Stock and Cisse. It contains 7 black and 3 white basketball players. They are similar to the color and also located in the air. They assumed that the ball play an important role in prediction. So, they ran another experiment with the same image but now cropping the image to remove the ball. Now the model predictated it as he is playing racket. Neighbor in this training class are white players. Image share certain charateristics, such as the background is green and most of the people are wearing white dresses and holding there hands in the air. In this example, besides the skin color, the position and appearance of the ball also contributed to the model&rsquo;s prediction.</p>
<h2 id="evaluation-of-robustness">Evaluation of Robustness</h2>
<p>DkNN is a step towards correctly handling malicious inputs like adversarial inputs because:</p>
<ul>
<li>outputs more reliable confidence estimates on adversarial examples than the softmax.</li>
<li>provides insights as to why adversarial examples affect undefended DNNs.</li>
<li>robust to adaptive attacks they considered</li>
</ul>
<h2 id="accuracy-on-adversarial-examples">Accuracy on Adversarial Examples</h2>
<p>They crafted adversarial examples using three algorithms: Fast Gradient Sign Method (FGSM), Basic Iterative Method (BIM), and Carlini-Wagner 2 attack (CW).</p>
<p>All there test results are shown in the following table. They have also included the accuracy of both undefended DNN and DkNN. By observing the table, they made a conclusion that even though the attacks were successful in evading the undefended DNN, but when the model is integrated with DkNN, then some accuracy on adversarial examples is recovered.</p>
<!-- raw HTML omitted -->
<p>They have plotted the reliability diagrams comparing the DkNN credibility on GTSRB adversarial examples with the softmax probabilities output by the DNN. For DkNN, credibility is low across all attacks. Here, the number of points in each bin is reflected by the red line. DkNN outputs a credibility below 0.5 for most of the inputs. This indicates a sharp departure from softmax probabilities, which classified most adversarial examples in the wrong class with a high confidence of above 0.9 for the FGSM and BIM attacks. They also made an observation that the BIM attack is more successful at introducing perturbations than that of the FGSM or the CW attacks.</p>
<!-- raw HTML omitted -->
<h2 id="explanation-of-dnn-mispredictions">Explanation of DNN Mispredictions</h2>
<!-- raw HTML omitted -->
<p>In the above figure, for both clean and adversarial examples, we can observe that the number of candidate labels decreases as we move up the neural network from its input layer all the way to its output layer. Number of candidate labels (in this case k = 75 nearest neighboring training representations) that match the final prediction made by the DNN is smaller for some attacks. For CW attack, the true label of adversarial examples that it produces is often recovered by the DkNN. Again, the lack of conformity between neighboring training representations at different layers of the model characterizes the weak support for the model’s prediction.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h3 id="comparison-to-lid">Comparison to LID</h3>
<p>We discussed similarities between the DkNN approach and Local
Intrinsic Dimensionality [<!-- raw HTML omitted -->ICLR 2018<!-- raw HTML omitted -->]. There
are important differences between the approaches, but given the
results reported in the <em>Obfuscated Gradients Give a False Sense of
Security: Circumventing Defenses to Adversarial Examples</em> (discussed
in <!-- raw HTML omitted -->Class 3<!-- raw HTML omitted -->) reported on
LID, it is worth investigating how robust DkNN is to the same
attacks. (Note that neither of these defenses are really obfuscating
gradients, so the attack strategy reported in that paper is to just use high confidence adversarial examples.)</p>
<h2 id="the-secret-sharer-measuring-unintended-neural-network-memorization--extracting-secrets">The Secret Sharer: Measuring Unintended Neural Network Memorization &amp; Extracting Secrets</h2>
<blockquote>
<p>Nicholas Carlini, Chang Liu, Jernej Kos, Ulfar Erlingsson, Dawn Song. 2018. The Secret Sharer: Measuring Unintended Neural Network Memorization &amp; Extracting Secrets.  arXiv:1802.08232. <a href="https://arxiv.org/pdf/1802.08232.pdf">[PDF]</a></p>
</blockquote>
<p>This paper focuses an adversary targeting &ldquo;secret&rdquo; user information stored in a deep neural network. Sensitive or &ldquo;secret&rdquo; user information can be included in the datasets used to train deep machine learning models. For example, if a model is trained on a dataset of emails, some of which have credit card numbers, there is a high probability that the credit card number can be extracted from the model, according to this paper.</p>
<h2 id="introduction">Introduction</h2>
<p>Rapid adoption of machine learning techniques has resulted in models trained on sensitive user information or &ldquo;secrets&rdquo;. The &ldquo;secrets&rdquo; may include “person messages, location histories, or medical information.” The potential for machine learning models to memorize or store secret information could reveal sensitive user information to an adversary. Even black-box models were found to be susceptible to leaking secret user information. As more deep learning models are implemented, we need to be mindful of the models ability to store information, and to shield models from revealing secrets.</p>
<h2 id="contributions">Contributions</h2>
<p>The exposure metric defined in this paper measures the ability of a model to memorize a secret. The higher the exposure of a model, the more likely a model is memorizing secret user information. The paper uses this metric to compare the ability of different models to memorize with different hyper-parameters. An important observation was that secrets were found to be memorized early in training rather than in the period of over-fitting. The author found that this property was consistent between different models and hyper parameters. Models included in paper focus on deep learning generative text models. Convolution neural network were also tested, but perform worse generally on text-based data. Extraction of secret information from the model was tested with varying hyper-parameters and conditions.</p>
<h2 id="perplexity-and-exposure">Perplexity and Exposure</h2>
<p>Perplexity is a measurement of how well a probability distribution predicts a sample. A model will have completely memorized the randomness of the training set if the log-perplexity of a secret is the absolute smallest.</p>
<p>Log-perplexity suggests memorization but does not yield general information about the extent of memorization in the model. Thus the authors define rank:</p>
<!-- raw HTML omitted -->
<p>To compute the rank, you must iterate over all possible secrets. To avoid this heavy computation the authors define multiple numerical methods to compute a value related to the rank of a secret, exposure.</p>
<!-- raw HTML omitted -->
<h2 id="secret-extraction-methods">Secret Extraction Methods</h2>
<p>The authors identify four methods for extracting secrets from a black-box model.</p>
<ol>
<li>Brute Force</li>
<li>Generative Sampling</li>
<li>Beam Search</li>
<li>Shortest Path Search</li>
</ol>
<p>Brute force was determined to be too computationally expensive, as the randomness space is too large. Generative sampling and beam search both fail to give the most optimal solution. The author&rsquo;s used shortest path search to guarantee the lowest log-perplexity solution. Their approach was based on Dijkstra&rsquo;s graph algorithm, and is explained in the paper. The figure below demonstrates the process of maximizing the log-perplexity for the purpose of finding the secret.</p>
<!-- raw HTML omitted -->
<h2 id="characterizing-memorization-of-secrets">Characterizing Memorization of Secrets</h2>
<p>In order to better understand model memorization the authors tested different numbers of iterations, various model architectures, multiple training strategies, changes in secret formats and context, and memorization across multiple simultaneous secrets.</p>
<h4 id="iterations">Iterations</h4>
<p>As showing in the figure below, the authors collected data relating to the exposure of the model throughout training. The exposure of the secret can clearly be seen rising until the test error starts to rise. This increase in exposure expresses the model&rsquo;s memorization of the training set early in the training process.</p>
<!-- raw HTML omitted -->
<h4 id="model-architectures">Model architectures</h4>
<p>The authors observed, through their exposure metric, that memorization was a shared property among recurrent and convolutional neural networks. Data relating to the different model architectures and exposure is shown in the figure below.</p>
<!-- raw HTML omitted -->
<h4 id="training-strategies">Training strategies</h4>
<p>As seen in the figure below, smaller batch sizes resulted in lower levels of memorization. Although having a smaller batch size slows down distributed processing of models, it is clear that their results suggest that a smaller batch size can reduce memorization.</p>
<!-- raw HTML omitted -->
<h4 id="secret-formats-and-context">Secret formats and context</h4>
<p>The table below interestingly suggests that the context of secrets significantly impacts whether an adversary can detect memorization. As there become more characters or information associated with the secret, the adversary has an easier time extracting randomness.</p>
<!-- raw HTML omitted -->
<h4 id="memorization-across-multiple-simultaneous-secrets">Memorization across multiple simultaneous secrets</h4>
<p>The table below shows the effect of inserting multiple secrets into the dataset. As the number of insertions increase, the model becomes more likely to memorize the secrets that were inserted.</p>
<!-- raw HTML omitted -->
<h2 id="evaluating-word-level-models">Evaluating word level models</h2>
<p>An interesting observation was that the capacity of a large word-level model produced better results than the smaller character-level model. By applying the exposure metrics to a word-level model, the authors found that representation of numbers mattered heavily in the memorization of information. When the authors replaced replaced &ldquo;1&rdquo; with &ldquo;one,&rdquo; they saw the exposure dropped by more than half from 25 to 12 for the large word-level model. Because the word-level model was more than 80 times the size of the character-level model, the authors found it surprising that &ldquo;[the large model] has sufficient capacity to memorize the training data completely, but it actually memorizes less.&rdquo;</p>
<h2 id="conclusion">Conclusion</h2>
<p>This paper examines the extent to which memorization has occurred in different types of algorithms and how sensitive user information can be revealed through this memorization. The metrics used in the paper can be easily transferred to existing models that have a well-defined notion of perplexity. They also demonstrate that these metrics can also be used to extract secret user information from black-box models.</p>
<p>&mdash; Team Panda: Christopher Geier, Faysal Hossain Shezan, Helen Simecek, Lawrence Hook, Nishant Jha</p>
<h3 id="sources">Sources</h3>
<blockquote>
<p>Kexin Pei, Yinzhi Cao, Junfeng Yang, Suman Jana. 2017. DeepXplore: Automated Whitebox Testing of Deep Learning Systems. In Proceedings of ACM Symposium on Operating Systems Principles (SOSP ’17). ACM, New York, NY, USA, 18 pages. https: //doi.org/10.1145/3132747.3132785 <a href="https://arxiv.org/pdf/1705.06640.pdf">[PDF]</a></p>
</blockquote>
<blockquote>
<p>Nicholas Carlini, Chang Liu, Jernej Kos, Ulfar Erlingsson, Dawn Song. 2018. The Secret Sharer: Measuring Unintended Neural Network Memorization &amp; Extracting Secrets.  arXiv:1802.08232. Retrieved from <a href="https://arxiv.org/pdf/1802.08232.pdf">https://arxiv.org/pdf/1802.08232.pdf</a> <a href="https://arxiv.org/pdf/1802.08232.pdf">[PDF]</a></p>
</blockquote>
<blockquote>
<p>Nicolas Papernot, Patrick McDaniel. 2018. Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning. Retrieved from <a href="https://arxiv.org/pdf/1803.04765.pdf">[PDF]</a></p>
</blockquote>

</div>


  <footer class="container">
  <div class="container navigation no-print">
  <h2>Navigation</h2>
  
  

    
    <a class="prev" href="https://secml.github.io/class7/" title="Class 7: Biases in ML, Discriminatory Advertising">
      Previous
    </a>
    

    
    <a class="next" href="https://secml.github.io/class9/" title="Class 9: Adversarial Malware Detection">
      Next
    </a>
    

  


</div>

</footer>

</article>
      <footer id="main-footer" class="container main_footer">
  

  <div class="container nav foot no-print">
  

  <a class="toplink" href="#">back to top</a>

</div>

  <div class="container credits">
  
<div class="container footline">
  
  <!-- raw HTML omitted -->


</div>


  

</div>

</footer>

    </main>
    




    
  </body>
</html>

